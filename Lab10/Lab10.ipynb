{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n",
    "\n",
    "Credits:\n",
    "\n",
    "* [Source](https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/TicTacToe/ticTacToe.py)\n",
    "* Code has been analyzed, fixed, commented\n",
    "\n",
    "Main changes: \n",
    "\n",
    "* Fixed diagonal win condition\n",
    "* Added possibility for human player to start first\n",
    "* Improved training time for better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tic Tac Toe\n",
    "---\n",
    "Two players against each other\n",
    "\n",
    "<img style=\"float:left\" src=\"board.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Board State\n",
    "---\n",
    "Reflect & Judge the state\n",
    "\n",
    "2 players p1 and p2; p1 uses symbol 1 and p2 uses symbol 2, vacancy as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    # game initialization: 1 empty board, 2 players\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first (p1->1   p2-> -1); \n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "    \n",
    "    # function to check if there is a winner\n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS-i-1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(diag_sum1, diag_sum2)\n",
    "        if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "            self.isEnd = True\n",
    "            return 1\n",
    "        if diag_sum1 == -3 or diag_sum2 == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "    \n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "    \n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "    \n",
    "    # only when game ends\n",
    "    # +1 given to the winning player\n",
    "    # +0.1 given to p1, +0.5 given to p2 in case of a tie -> since p1 starts first, it looks fair to give more ponts to p2\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(0)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0.1)\n",
    "            self.p2.feedReward(0.5)\n",
    "    \n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "    \n",
    "    #training games\n",
    "    def play(self, rounds=100):\n",
    "        for i in range(rounds):\n",
    "            if i%1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "                    \n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "    \n",
    "    # play with human; Computer starts first\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "        \n",
    "    # play with human; Human starts first\n",
    "    def play3(self):\n",
    "        self.showBoard()\n",
    "\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    # init player\n",
    "    # exp_rate balance between exploration and exploitation\n",
    "    # 30% random move -> exploration\n",
    "    # 70% best move -> exploitation\n",
    "    def __init__(self, name, exp_rate=0.3):\n",
    "        self.name = name\n",
    "        self.states = []    # record all positions taken during a game for the training process\n",
    "        self.lr = 0.2       # step length\n",
    "        self.exp_rate = exp_rate\n",
    "        self.discount = 0.9     # gamma is high -> winning the game is more important than the immediate reward\n",
    "        self.states_value = {}  # record all positions with their correspondive score\n",
    "    \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS*BOARD_ROWS))\n",
    "        return boardHash\n",
    "    \n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        # it's picked the board that grants the best scenario after the move is made\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    # Q learning formula\n",
    "    # if it's the first time we found that state 0 is assigned\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr*(self.discount*reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        \n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file,'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...takes around 13 minutes to train, feel free to take a coffee :D\n",
      "Rounds 0\n",
      "Rounds 1000\n",
      "Rounds 2000\n",
      "Rounds 3000\n",
      "Rounds 4000\n",
      "Rounds 5000\n",
      "Rounds 6000\n",
      "Rounds 7000\n",
      "Rounds 8000\n",
      "Rounds 9000\n",
      "Rounds 10000\n",
      "Rounds 11000\n",
      "Rounds 12000\n",
      "Rounds 13000\n",
      "Rounds 14000\n",
      "Rounds 15000\n",
      "Rounds 16000\n",
      "Rounds 17000\n",
      "Rounds 18000\n",
      "Rounds 19000\n",
      "Rounds 20000\n",
      "Rounds 21000\n",
      "Rounds 22000\n",
      "Rounds 23000\n",
      "Rounds 24000\n",
      "Rounds 25000\n",
      "Rounds 26000\n",
      "Rounds 27000\n",
      "Rounds 28000\n",
      "Rounds 29000\n",
      "Rounds 30000\n",
      "Rounds 31000\n",
      "Rounds 32000\n",
      "Rounds 33000\n",
      "Rounds 34000\n",
      "Rounds 35000\n",
      "Rounds 36000\n",
      "Rounds 37000\n",
      "Rounds 38000\n",
      "Rounds 39000\n",
      "Rounds 40000\n",
      "Rounds 41000\n",
      "Rounds 42000\n",
      "Rounds 43000\n",
      "Rounds 44000\n",
      "Rounds 45000\n",
      "Rounds 46000\n",
      "Rounds 47000\n",
      "Rounds 48000\n",
      "Rounds 49000\n",
      "Rounds 50000\n",
      "Rounds 51000\n",
      "Rounds 52000\n",
      "Rounds 53000\n",
      "Rounds 54000\n",
      "Rounds 55000\n",
      "Rounds 56000\n",
      "Rounds 57000\n",
      "Rounds 58000\n",
      "Rounds 59000\n",
      "Rounds 60000\n",
      "Rounds 61000\n",
      "Rounds 62000\n",
      "Rounds 63000\n",
      "Rounds 64000\n",
      "Rounds 65000\n",
      "Rounds 66000\n",
      "Rounds 67000\n",
      "Rounds 68000\n",
      "Rounds 69000\n",
      "Rounds 70000\n",
      "Rounds 71000\n",
      "Rounds 72000\n",
      "Rounds 73000\n",
      "Rounds 74000\n",
      "Rounds 75000\n",
      "Rounds 76000\n",
      "Rounds 77000\n",
      "Rounds 78000\n",
      "Rounds 79000\n",
      "Rounds 80000\n",
      "Rounds 81000\n",
      "Rounds 82000\n",
      "Rounds 83000\n",
      "Rounds 84000\n",
      "Rounds 85000\n",
      "Rounds 86000\n",
      "Rounds 87000\n",
      "Rounds 88000\n",
      "Rounds 89000\n",
      "Rounds 90000\n",
      "Rounds 91000\n",
      "Rounds 92000\n",
      "Rounds 93000\n",
      "Rounds 94000\n",
      "Rounds 95000\n",
      "Rounds 96000\n",
      "Rounds 97000\n",
      "Rounds 98000\n",
      "Rounds 99000\n",
      "Rounds 100000\n",
      "Rounds 101000\n",
      "Rounds 102000\n",
      "Rounds 103000\n",
      "Rounds 104000\n",
      "Rounds 105000\n",
      "Rounds 106000\n",
      "Rounds 107000\n",
      "Rounds 108000\n",
      "Rounds 109000\n",
      "Rounds 110000\n",
      "Rounds 111000\n",
      "Rounds 112000\n",
      "Rounds 113000\n",
      "Rounds 114000\n",
      "Rounds 115000\n",
      "Rounds 116000\n",
      "Rounds 117000\n",
      "Rounds 118000\n",
      "Rounds 119000\n",
      "Rounds 120000\n",
      "Rounds 121000\n",
      "Rounds 122000\n",
      "Rounds 123000\n",
      "Rounds 124000\n",
      "Rounds 125000\n",
      "Rounds 126000\n",
      "Rounds 127000\n",
      "Rounds 128000\n",
      "Rounds 129000\n",
      "Rounds 130000\n",
      "Rounds 131000\n",
      "Rounds 132000\n",
      "Rounds 133000\n",
      "Rounds 134000\n",
      "Rounds 135000\n",
      "Rounds 136000\n",
      "Rounds 137000\n",
      "Rounds 138000\n",
      "Rounds 139000\n",
      "Rounds 140000\n",
      "Rounds 141000\n",
      "Rounds 142000\n",
      "Rounds 143000\n",
      "Rounds 144000\n",
      "Rounds 145000\n",
      "Rounds 146000\n",
      "Rounds 147000\n",
      "Rounds 148000\n",
      "Rounds 149000\n",
      "Rounds 150000\n",
      "Rounds 151000\n",
      "Rounds 152000\n",
      "Rounds 153000\n",
      "Rounds 154000\n",
      "Rounds 155000\n",
      "Rounds 156000\n",
      "Rounds 157000\n",
      "Rounds 158000\n",
      "Rounds 159000\n",
      "Rounds 160000\n",
      "Rounds 161000\n",
      "Rounds 162000\n",
      "Rounds 163000\n",
      "Rounds 164000\n",
      "Rounds 165000\n",
      "Rounds 166000\n",
      "Rounds 167000\n",
      "Rounds 168000\n",
      "Rounds 169000\n",
      "Rounds 170000\n",
      "Rounds 171000\n",
      "Rounds 172000\n",
      "Rounds 173000\n",
      "Rounds 174000\n",
      "Rounds 175000\n",
      "Rounds 176000\n",
      "Rounds 177000\n",
      "Rounds 178000\n",
      "Rounds 179000\n",
      "Rounds 180000\n",
      "Rounds 181000\n",
      "Rounds 182000\n",
      "Rounds 183000\n",
      "Rounds 184000\n",
      "Rounds 185000\n",
      "Rounds 186000\n",
      "Rounds 187000\n",
      "Rounds 188000\n",
      "Rounds 189000\n",
      "Rounds 190000\n",
      "Rounds 191000\n",
      "Rounds 192000\n",
      "Rounds 193000\n",
      "Rounds 194000\n",
      "Rounds 195000\n",
      "Rounds 196000\n",
      "Rounds 197000\n",
      "Rounds 198000\n",
      "Rounds 199000\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"p1\")\n",
    "p2 = Player(\"p2\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"training...takes around 13 minutes to train, feel free to take a coffee :D\")\n",
    "st.play(200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.savePolicy()\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = Player(\"computer\", exp_rate=0)\n",
    "p2.loadPolicy(\"policy_p2\")\n",
    "#sorted(p2.states_value.items(), key=lambda e: e[1], reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer(p1) vs Human(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   | o | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| o |   | o | \n",
      "-------------\n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "|   | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "-------------\n",
      "|   | o | x | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "-------------\n",
      "| x | o | x | \n",
      "-------------\n",
      "| o | x | x | \n",
      "-------------\n",
      "| o | x | o | \n",
      "-------------\n",
      "tie!\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"computer\", exp_rate=0) # we have already trained our model, exploration=0 during testing\n",
    "p1.loadPolicy(\"policy_p1\") # policy_p1 is the trained agent when the player starts first\n",
    "\n",
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.play2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human(p1) vs Computer(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "|   | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| x |   |   | \n",
      "-------------\n",
      "| o | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "| o | o |   | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "-------------\n",
      "| x |   | x | \n",
      "-------------\n",
      "| o | o | o | \n",
      "-------------\n",
      "|   |   | x | \n",
      "-------------\n",
      "computer wins!\n"
     ]
    }
   ],
   "source": [
    "p2 = Player(\"computer\", exp_rate=0) \n",
    "p2.loadPolicy(\"policy_p2\") #policy_p2 is the trained agent when the computer starts second\n",
    "\n",
    "p1 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.play3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
